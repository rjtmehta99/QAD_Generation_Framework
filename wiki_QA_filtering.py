import helper
from numpy import nan
import pandas as pd
from haystack.nodes import FARMReader
from haystack.schema import Document
from haystack.utils import clean_wiki_text
from haystack.document_stores import ElasticsearchDocumentStore
from haystack import Pipeline

# GenerativeQAPipeline
# ExtractiveQAPipeline with and without BM25 
# preprocess docs
# With question summary 
# With question sections
# With answer summary 
# With answer sections

class WikiQA():
    def __init__(self, wiki_csv: str, index: str, target_column: str):
        self.wiki_csv = wiki_csv
        self.index = index
        self.target_column = target_column
            
    def _process_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        # Target column can be question/answer + summary/section
        df = pd.read_csv(self.wiki_csv)
        df = df.reset_index()
        df = df[['index', 'generated_question', 'generated_answer', self.target_column]]    
        df = (df
              .fillna('No content')
              .replace(nan, 'No content'))
        df['meta'] = df.apply(lambda row: {'question': row['generated_question'],
                                           'answer': row['generated_answer'],
                                           'index': row['index']},
                                           axis=1).to_list()
        df['content'] = df[self.target_column]
        df = df[['content', 'meta']]
        # By default id is generated by hashing only content which might be repeated
        # So it is changed to combination of meta and content
        df['id_hash_keys'] = [['meta', 'content']]*len(df)
        #dict_df = df.to_dict('records')
        return df

    def prepare_csv(self) -> dict:
        df = pd.read_csv(self.wiki_csv)
        processed_df = self._process_dataframe(df)
        dict_df = processed_df.to_dict('records')
        return dict_df

    def _load_docs(self, data: dict) -> tuple[ElasticsearchDocumentStore, list[Document]]:
        doc_store = helper.add_to_docstore(docs=data, index=self.index, delete_docs=True)
        docs = helper.load_all_docs(topic=self.index)
        return doc_store, docs

    def prepare_query_doc_pairs(self, data):
        doc_store, docs = self._load_docs(data=data)
        num_docs = len(docs)
        # List of questions, to be used as queries, index-wise
        self.queries = [docs[i].meta['question'] for i in range(num_docs)]
        # List of documents, index-wise
        self.filtered_docs = [doc_store.get_all_documents(index=self.index, 
                                                          filters={'index': i}) for i in range(num_docs)]
        # List of actual answer, to be used for evaluation later
        self.actual_answers = [docs[i].meta['answer'] for i in range(num_docs)]
        #return queries, filtered_docs, actual_answers

    def create_reader(self, qa_model: str = 'deepset/roberta-base-squad2'):
        reader = FARMReader(model_name_or_path=qa_model,
                                 context_window_size=250,
                                 max_query_length=128,
                                 doc_stride = 72,
                                 top_k=2, use_gpu=True)
        return reader

    def build_pipeline(self, reader):
        pipeline = Pipeline()
        pipeline.add_node(component=reader, name='Reader', inputs=['Query'])
        return pipeline
        #results = pipeline.run_batch(queries=self.qu)

    def execute_pipeline(self, pipeline):
        #self.prepare_query_doc_pairs()
        results = pipeline.run_batch(queries=self.queries, documents=self.filtered_docs, debug=True)
        return results
    
    def save_results(self, results):        
        wiki_ans = []
        wiki_score = []
        for val in results['answers']:
            wiki_ans.append([ans.answer for ans in val])
            wiki_score.append([ans.score for ans in val])
        df_wiki = pd.DataFrame(data={'generated_question': self.queries,
                                     'generated_answer': self.actual_answers,
                                     'wiki_answers': wiki_ans,
                                     'wiki_scores': wiki_score})
        df_wiki.to_csv(f'data/{self.target_column}_QA.csv', index=False)

wikiqa = WikiQA(wiki_csv='data/generated_QA_wiki.csv', 
                index='wiki_question_summary', 
                target_column='question_wiki_summary')

dict_df = wikiqa.prepare_csv()
wikiqa.prepare_query_doc_pairs(data=dict_df)

reader = wikiqa.create_reader()
pipeline = wikiqa.build_pipeline(reader=reader)
results = wikiqa.execute_pipeline(pipeline=pipeline)
wikiqa.save_results(results)
